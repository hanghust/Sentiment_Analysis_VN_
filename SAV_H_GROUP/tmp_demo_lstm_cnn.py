# -*- coding: utf-8 -*-
"""tmp_Demo_LSTM_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f8vXZlU-SQaubNCe4NV6o_ZgH2vnQmMo
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/Do_An_III/Sentiment_Analysis_Vietnamese'
! ls

"""1.   Tiền xử lý dữ liệu sau đó lưu dữ liệu vào file .npy tương ứng"""

import numpy as np
import re

word2id= {}
id2word={}
index = 1
maxlen = 0
avglen = 0
count100 = 0
#read file
train_pos_file = "Data/training_data/train_pos.txt"
train_neg_file = "Data/training_data/train_neg.txt"

test_pos_file = "Data/testing_data/test_pos.txt"
test_neg_file = "Data/testing_data/test_neg.txt"

val_pos_file = "Data/validation_data/val_pos.txt"
val_neg_file = "Data/validation_data/val_neg.txt"


open_files = [train_pos_file, train_neg_file, test_pos_file, test_neg_file, val_pos_file, val_neg_file]

#save file
train_pos_save = "Data/training_data/train_pos"
train_neg_save = "Data/training_data/train_neg"

test_pos_save = "Data/testing_data/test_pos"
test_neg_save = "Data/testing_data/test_neg"

val_pos_save = "Data/validation_data/val_pos"
val_neg_save = "Data/validation_data/val_neg"

save_files = [train_pos_save, train_neg_save, test_pos_save, test_neg_save, val_pos_save, val_neg_save]

for open_file, save_file in zip(open_files,save_files):
  pos = []
  file = open(open_file, 'r')

  for aline in file.readlines():
      aline = aline.replace('\n', "") 
      ids = np.array([], dtype='int32')
      for word in aline.split(' '):
          word = word.lower()
          if word in word2id:
              ids = np.append(ids, word2id[word])
          else:
              if word != '':
                  # print (word, "not in vocalbulary")
                  word2id[word] = index
                  id2word[index] = word
                  ids = np.append(ids, index)
                  index = index + 1
      if len(ids) > 0:
          pos.append(ids)

  file.close()
  print(len(pos))
  np.save(save_file, pos)
  for li in pos:
      if maxlen < len(li):
          maxlen = len(li)
      avglen += len(li)
      if len(li) > 400:
          count100+=1

print(len(word2id))
print(word2id)
print(ids)
print ("maxlen",maxlen)
print ("maxlen400",count100)

import os
import pickle
import _pickle

"""1.   Gán lable tương ứng cho từng câu tương ứng đã qua bước tiền xử lý :


*   label = [1, 0] => tương ứng với câu tích cực
*   label = [0, 1] => tương ứng với câu tiêu cực
"""

def load_data_shuffle():

    train_pos_save = "Data/training_data/train_pos.npy"
    train_neg_save = "Data/training_data/train_neg.npy"

    test_pos_save = "Data/testing_data/test_pos.npy"
    test_neg_save = "Data/testing_data/test_neg.npy"

    val_pos_save = "Data/validation_data/val_pos.npy"
    val_neg_save = "Data/validation_data/val_neg.npy"
    # save np.load
    np_load_old = np.load

    # modify the default parameters of np.load
    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)
    #Load train data
    pos_train = np.load(train_pos_save)
    neg_train = np.load(train_neg_save)

    y_pos_train = []
    for i in pos_train:
        y_pos_train.append([1,0])
    y_pos_train = np.array(y_pos_train)

    y_neg_train = []
    for i in neg_train:
        y_neg_train.append([0, 1])
    y_neg_train = np.array(y_neg_train)



    #load test data
    pos_test = np.load(test_pos_save)
    neg_test = np.load(test_neg_save)

    y_pos_test = []
    for i in pos_test:
        y_pos_test.append([1,0])
    y_pos_test = np.array(y_pos_test)

    y_neg_test = []
    for i in neg_test:
        y_neg_test.append([0, 1])
    y_neg_test = np.array(y_neg_test)


        #load val data
    pos_val = np.load(val_pos_save)
    neg_val = np.load(val_neg_save)

    y_pos_val = []
    for i in pos_val:
        y_pos_val.append([1,0])
    y_pos_val = np.array(y_pos_val)

    y_neg_val = []
    for i in neg_val:
        y_neg_val.append([0, 1])
    y_neg_val = np.array(y_neg_val)

    # restore np.load for future normal usage
    np.load = np_load_old

    X_train = np.concatenate([pos_train, neg_train])
    y_train = np.concatenate([y_pos_train, y_neg_train])

    X_val = np.concatenate([pos_val, neg_val])
    y_val = np.concatenate([y_pos_val, y_neg_val])

    X_test = np.concatenate([pos_test, neg_test])
    y_test = np.concatenate([y_pos_test, y_neg_test])

    print (X_train.shape, y_train.shape)
    print (X_test.shape, y_test.shape)
    print (X_val.shape, y_val.shape)

    return X_train, y_train, X_test, y_test, X_val, y_val

from __future__ import print_function
from socket import socket
from keras.layers.merge import concatenate

np.random.seed(1337)  # for reproducibility

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta
# from load_data import load_data_shuffle
from keras.preprocessing import sequence as sq
from keras.layers import Dense, Dropout, Activation, Lambda,merge,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

import keras.backend.tensorflow_backend as K
from keras.callbacks import LearningRateScheduler

"""biểu diễn độ dài của tất cả các câu reviews \
=> hầu hết dữ liệu có đọ dài nằm khoảng (250, 400) \
rất ít review có độ dài vượt quá 400 \
 => chọn maxlen = 400 (có một vài dữ liệu bị mất mát thông tin nhưng không đáng kể )
"""

# from keras.callbacks import LearningRateScheduler

#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \
#                        allow_soft_placement=True, device_count = {'CPU': 1})


# tf_config = K.tf.ConfigProto()
# tf_config.gpu_options.allow_growth = True
# session = K.tf.Session(config=tf_config)
# K.set_session(session)

# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \
#                         allow_soft_placement=True, device_count = {'CPU': 4})
# session = K.tf.Session(config=config)
# K.set_session(session)

# set parameters:


print('Loading data ...')

X_train, y_train, X_test, y_test, X_val, y_val = load_data_shuffle()
print(len(X_train), 'train sequences')
print(len(X_test), 'test sequences')
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
line = []
# num_bins = len(X_train)
# n, bins, patches = plt.hist(X_train, num_bins, normed=1, facecolor='blue', alpha=0.5)
for x in X_train:
  line.append(len(x))
plt.plot(line, 'r--')
plt.xlabel('câu')
plt.ylabel('độ dài câu')
plt.title(r'Biểu đồ phân bổ độ dài của câu')

# Tweak spacing to prevent clipping of ylabel
plt.subplots_adjust(left=0.15)
plt.show()

max_features = 41375 #21540#14300 
maxlen = 400
batch_size = 1000
embedding_dims = 150
nb_filter = 150
filter_length = 3
hidden_dims = 100

accs = []

X_train[5]

X_train = sq.pad_sequences(X_train, maxlen=maxlen)
X_test = sq.pad_sequences(X_test, maxlen=maxlen)
X_val = sq.pad_sequences(X_val, maxlen=maxlen)
print('X_train shape:', X_train.shape)
print('X_val shape:', X_val.shape)
print('X_test shape:', X_test.shape)

def lr_schedule(epoch):
    lrate = 1.0
    if epoch > 2:
        lrate = 0.8
    if epoch > 18:
        lrate = 0.7
    if epoch > 30:
        lrate = 0.5
    if epoch > 35:
        lrate = 0.1
    return lrate

nb_epoch = 45

from keras.preprocessing.image import ImageDataGenerator

print('Build model...')
model = Sequential()

input_layer = Input(shape=(maxlen,),dtype='int32', name='main_input')
emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen
                      )(input_layer)
def max_1d(X):
    return K.max(X, axis=1)

# we add a Convolution1D, which will learn nb_filter
# word group filters of size 3:

con3_layer = Convolution1D(nb_filter=nb_filter,
                    filter_length=3,
                    border_mode='valid',
                    activation='relu',
                    subsample_length=1)(emb_layer)

pool_con3_layer = Lambda(max_1d, output_shape=(nb_filter,))(con3_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 4:

con4_layer = Convolution1D(nb_filter=nb_filter,
                    filter_length=5,
                    border_mode='valid',
                    activation='relu',
                    subsample_length=1)(emb_layer)

pool_con4_layer = Lambda(max_1d, output_shape=(nb_filter,))(con4_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 5:

con5_layer = Convolution1D(nb_filter=nb_filter,
                    filter_length=7,
                    border_mode='valid',
                    activation='relu',
                    subsample_length=1)(emb_layer)

pool_con5_layer = Lambda(max_1d, output_shape=(nb_filter,))(con5_layer)


# cnn_layer = merge([pool_con3_layer, pool_con5_layer,pool_con4_layer ], mode='concat')
cnn_layer = concatenate([pool_con3_layer, pool_con4_layer,pool_con5_layer])
# cnn_layer = concatenate([pool_con3_layer,pool_con4_layer])


#LSTM


x = Embedding(max_features, embedding_dims, input_length=maxlen)(input_layer)
lstm_layer = LSTM(128)(x)

cnn_lstm_layer = concatenate([lstm_layer, cnn_layer])

dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)
output_layer= Dropout(0.2)(dense_layer)
output_layer = Dense(2, trainable=True,activation='softmax')(output_layer)




model = Model(input=[input_layer], output=[output_layer])
adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-06)


model.compile(loss='categorical_crossentropy',
              optimizer=adadelta,
              metrics=['accuracy'])
model.summary()
checkpoint = ModelCheckpoint('Model/CNN-LSTM-weights/weights.hdf5',
                              monitor='val_acc', verbose=0, save_best_only=True,
                              mode='max')


history = model.fit(X_train, y_train, batch_size=batch_size,\
          # steps_per_epoch=len(X_train) / batch_size,\
          # validation_steps = len(X_train)/batch_size,
          epochs=nb_epoch,\
          # callbacks=[checkpoint],\
          validation_data=(X_val, y_val),\
          callbacks=[LearningRateScheduler(lr_schedule,verbose=1), checkpoint])

"""Đồ thị biểu diễn quá trình huấn luyện modol với epoch = 45
max_features = 41375 \
maxlen = 400  \
batch_size = 1000  \
embedding_dims = 150  \
nb_filter = 150  \
filter_length = 3  \
learning_rate = 1.0  \
decay = 1e-02/epoch
"""

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train_acc', 'Val_acc'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['loss', 'val_loss'], loc='upper left')
plt.show()

"""Đồ thị biểu diễn quá trình huấn luyện modol với :  \
epoch = 45 \
max_features = 41375 \
maxlen = 400 \
batch_size = 1000 \
embedding_dims = 150 \
nb_filter = 150  \
hidden_dims = 100 learning_rate cài đặt bằng tay thông qua hàm :\
def lr_schedule(epoch): \
    

*   List item

    lrate = 1.0 \
    if epoch > 2:  
        lrate = 0.8  
    if epoch > 18:  
        lrate = 0.7  
    if epoch > 30:  
        lrate = 0.5  
    if epoch > 35:  
        lrate = 0.1  
    return lrate
"""

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train_acc', 'Val_acc'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['loss', 'val_loss'], loc='upper left')
plt.show()

y_pred = model.predict(X_test)

y_pred = np.array(y_pred).round()
y_pred.astype(int)
y_test = np.array(y_test)

count_pos = 0
count_neg = 0
y_acc_neg = 0
y_acc_pos = 0
for elm in y_pred:
  if elm[0] == 1:
    count_pos +=1
  else:
    count_neg +=1
for elm in y_test:
  if elm[0] == 1:
    y_acc_pos +=1
  else:
    y_acc_neg +=1
print(count_pos)
print(count_neg)

# Data to plot
labels = ['positive', 'negative']
sizes = [count_pos, count_neg]
sizes1 = [y_acc_pos, y_acc_neg]
colors = ['gold', 'yellowgreen']
explode = (0.01, 0)  # explode 1st slice

# Plot
plt.title('Kết quả phân tích')
plt.pie(sizes, explode=explode, labels=labels, colors=colors,
autopct='%1.1f%%', shadow=True, startangle=140)

plt.axis('equal')
plt.show()
# Plot
plt.title('Kết quả thực tế')
plt.pie(sizes1, explode=explode, labels=labels, colors=colors,
autopct='%1.1f%%', shadow=True, startangle=140)

plt.axis('equal')
plt.show()

"""kết quả huấn luyện với lr tự cài đặt bằng tay"""

from sklearn import metrics
# Model accuracy: how often is the classifier correct?
print("Accuracy: ", 100*metrics.accuracy_score(y_test, y_pred))

print("Precision: ", 100*metrics.precision_score(y_test, y_pred,average='micro'))

print("Recall (macro): ", 100*metrics.recall_score(y_test, y_pred, average='macro'))


print("Recall(micro): ", 100*metrics.recall_score(y_test, y_pred, average='micro'))zzzzzzzzzzzzzzzzzzzzzzzaaaaaaaaaaaaa

print("F1-scores(macro): ", 100*metrics.f1_score(y_test, y_pred, average='macro'))

print("F1-scores(micro): ", 100*metrics.f1_score(y_test, y_pred, average='micro'))

#save to disk
model_json = model.to_json()
with open('Model/model.json', 'w') as json_file:
    json_file.write(model_json)
model.save_weights('Model/model.h5')

"""kết quả huấn luyện với lr sử dụng decay của keras"""

from sklearn import metrics
# Model accuracy: how often is the classifier correct?
print("Accuracy: ", 100*metrics.accuracy_score(y_test, y_pred))

print("Precision: ", 100*metrics.precision_score(y_test, y_pred,average='micro'))

print("Recall (macro): ", 100*metrics.recall_score(y_test, y_pred, average='macro'))


print("Recall(micro): ", 100*metrics.recall_score(y_test, y_pred, average='micro'))

print("F1-scores(macro): ", 100*metrics.f1_score(y_test, y_pred, average='macro'))

print("F1-scores(micro): ", 100*metrics.f1_score(y_test, y_pred, average='micro'))

# Code load model to test
from keras.models import load_model
from keras.models import model_from_json
# load json and create model
json_file = open('Model/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("Model/model.h5")
print("Loaded model from disk")
scores = model.evaluate(X_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])